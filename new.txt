import nltk
from nltk.tree import Tree

# Build the parse tree manually based on the grammar and sentence structure
parse_tree = Tree('S', [
    Tree('NP', [
        Tree('Det', ['the']),
        Tree('Nom', [
            Tree('Adj', ['angry']),
            Tree('Nom', [
                Tree('N', ['bear'])
            ])
        ])
    ]),
    Tree('VP', [
        Tree('V', ['chased']),
        Tree('NP', [
            Tree('Det', ['the']),
            Tree('Nom', [
                Tree('Adj', ['frightened']),
                Tree('Nom', [
                    Tree('Adj', ['little']),
                    Tree('Nom', [
                        Tree('N', ['squirrel'])
                    ])
                ])
            ])
        ])
    ])
])

# Pretty-print the parse tree
parse_tree.pretty_print()

-----------------------------------------------------------------------------
def min_edit_distance(str1, str2):
    m = len(str1)
    n = len(str2)

    # Create a 2D DP table
    dp = [[0 for _ in range(n + 1)] for _ in range(m + 1)]

    # Fill the base cases
    for i in range(m + 1):
        dp[i][0] = i  # delete all characters from str1
    for j in range(n + 1):
        dp[0][j] = j  # insert all characters to str1

    # Fill the rest of the table
    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if str1[i - 1] == str2[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]  # no operation needed
            else:
                dp[i][j] = 1 + min(
                    dp[i - 1][j],    # delete
                    dp[i][j - 1],    # insert
                    dp[i - 1][j - 1] # replace
                )

    return dp[m][n]

# Example usage
str1 = "Saturday"
str2 = "Sunday"

operations = min_edit_distance(str1, str2)
print("Minimum edit distance:", operations)

-----------------------------------------------------------------------------
import nltk
from nltk.util import ngrams
from collections import Counter, defaultdict

# Download if not already
nltk.download('punkt')

# Given corpus
corpus = [
    "<S> I am Henry </S>",
    "<S> I like college </S>",
    "<S> Do Henry like college </S>",
    "<S> Henry I am </S>",
    "<S> Do I like Henry </S>",
    "<S> Do I like college </S>",
    "<S> I do like Henry </S>"
]

# Tokenize and build 4-grams
four_grams = []
for sentence in corpus:
    tokens = sentence.split()
    four_grams += list(ngrams(tokens, 4))

# Count 4-grams that start with ('Do', 'I', 'like')
context = ('Do', 'I', 'like')
next_word_freq = Counter()

for gram in four_grams:
    if gram[:3] == context:
        next_word_freq[gram[3]] += 1

# Display result
if next_word_freq:
    print("Most probable next word:", next_word_freq.most_common(1)[0][0])
    print("All options:", dict(next_word_freq))
else:
    print("No match found for context:", context)

-----------------------------------------------------------------------------
import nltk
from nltk.stem import (
    PorterStemmer,
    LancasterStemmer,
    SnowballStemmer,
    RegexpStemmer,
    WordNetLemmatizer
)
from textblob import Word as TextBlobWord

# Download required data
# nltk.download('punkt')
# nltk.download('wordnet')
# nltk.download('omw-1.4')

# Input words
words = ['Programming', 'Loving', 'Lovely', 'Kind']

# Initialize stemmers
porter = PorterStemmer()
lancaster = LancasterStemmer()
snowball = SnowballStemmer("english")
regexp = RegexpStemmer('ing$|ly$', min=4)

# Lemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

print(f"{'Word':<15} {'Porter':<12} {'Lancaster':<12} {'Snowball':<12} {'Regexp':<12} {'WordNet Lemma':<16} {'TextBlob Lemma'}")
print("-" * 100)

for word in words:
    w = word.lower()
    textblob_word = TextBlobWord(w)
    print(f"{word:<15} "
          f"{porter.stem(w):<12} "
          f"{lancaster.stem(w):<12} "
          f"{snowball.stem(w):<12} "
          f"{regexp.stem(w):<12} "
          f"{wordnet_lemmatizer.lemmatize(w, pos='v'):<16} "
          f"{textblob_word.lemmatize()}")


-----------------------------------------------------------------------------
import nltk

sentences = [
    "I need a flight from Atlanta.",
    "Everything to permit us.",
    "I would like to address the public on this issue.",
    "We need your shipping address."
]

for sentence in sentences:
    tokens = sentence.split()
    tagged = nltk.pos_tag(tokens)
    print(f"\nSentence: {sentence}")
    print("Tagged:", tagged)

-----------------------------------------------------------------------------
paragraph = """
Artificial Intelligence (AI) has rapidly evolved over the past decade, becoming a significant force across industries.
From healthcare to finance, AI systems are revolutionizing how tasks are performed.
In medicine, AI assists in early detection of diseases and personalized treatments.
In the automotive sector, self-driving technology powered by AI has made tremendous strides.
AI chatbots now handle customer service queries 24/7 with impressive accuracy.
However, with great power comes great responsibility.
There are concerns about privacy, ethics, and job displacement caused by AI systems.
Regulation is essential to ensure AI is used fairly and transparently.
Education systems are adapting to teach AI concepts at an early age.
Looking ahead, AI is expected to become more human-like, capable of understanding emotions and context.
But itâ€™s crucial that its development remains aligned with human values and benefit to society.
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
from collections import defaultdict
import heapq

# # Download required resources
# nltk.download('punkt')
# nltk.download('stopwords')

# Tokenize into sentences
sentences = sent_tokenize(paragraph)
words = word_tokenize(paragraph.lower())

# Filter stopwords and punctuation
stop_words = set(stopwords.words('english'))
filtered_words = [word for word in words if word.isalnum() and word not in stop_words]

# Frequency table
word_freq = defaultdict(int)
for word in filtered_words:
    word_freq[word] += 1

# Sentence scores
sent_scores = defaultdict(int)
for sent in sentences:
    for word in word_tokenize(sent.lower()):
        if word in word_freq:
            sent_scores[sent] += word_freq[word]

# Extract top 3 scored sentences
summary_sentences = heapq.nlargest(3, sent_scores, key=sent_scores.get)
summary = " ".join(summary_sentences)

print("ðŸ”¹ Extraction-based Summary:\n", summary)

-----------------------------------------------------------------------------
import re

text = """
You can reach us at contact@company.com or sales@example.org.
Alternatively, email john.doe99@gmail.com for support.
Call us on 9876543210 or +91-9123456789 anytime.
"""

names = ['Sunil', 'Shyam', 'Ankit', 'Surjeet', 'Sumit', 'Subhi', 'Surbhi', 'Siddharth', 'Sujan']

# Simplified email regex (still matches valid emails)
emails = re.findall(r'[\w.-]+@[\w.-]+\.\w+', text)
print("Emails:", emails)

# Simplified phone regex (10 digits with optional +91 or +91-)
phones = re.findall(r'(?:\+91[-\s]?)?\d{10}', text)
print("Phone Numbers:", phones)

# Names starting with 'Su' and 5 letters total (Su___)
pattern_names = [name for name in names if re.fullmatch(r'Su\w{3}', name)]
print("Matching Names (Su___):", pattern_names)

sample_text = "abc132 abc abc123"

# Find first sequence of digits
search_result = re.search(r'\d+', sample_text)
print("Search (first digits):", search_result.group())

# Match 'abc' at beginning
match_result = re.match(r'abc', sample_text)
print("Match at start:", match_result.group())

# Replace all 'abc' with 'XYZ'
sub_result = re.sub(r'abc', 'XYZ', sample_text)
print("Substitution:", sub_result)

# Precompiled pattern
pattern = re.compile(r'abc')
compiled_result = pattern.findall(sample_text)
print("Compiled pattern findall:", compiled_result)

sample = "ab abccc abbbbb abccccc"
# 'ab' followed by 0 or more 'c'
matches = re.findall(r'abc*', sample)
print("Pattern 'ab' followed by 0+ 'c':", matches)

sample = "a abc abcbc abcbcbc ab bcbc"
# 'a' followed by 0 or more 'bc'
matches = re.findall(r'a(?:bc)*', sample)
print("Pattern 'a' followed by 0+ 'bc':", matches)

sample = "ab abc abbc abb abcd"
# 'ab' followed by optional 'c'
matches = re.findall(r'ab(c?)', sample)
print("Pattern 'ab' followed by 0 or 1 'c':", matches)


-----------------------------------------------------------------------------